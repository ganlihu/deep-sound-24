[2025-09-13 14:30:00] ============================================================
[2025-09-13 14:30:00] 【训练数据信息】
[2025-09-13 14:30:00] 原始X类型: <class 'list'>, 长度: 1
[2025-09-13 14:30:00] 原始y类型: <class 'list'>, 长度: 42
[2025-09-13 14:30:00] ============================================================
[2025-09-13 14:30:00] 
===== 转换样本为NumPy数组 =====
[2025-09-13 14:30:00] 样本0：已从list转换为数组，形状=(288, 1800, 1)
[2025-09-13 14:30:00] 样本1：已从list转换为数组，形状=(227, 1800, 1)
[2025-09-13 14:30:00] 样本2：已从list转换为数组，形状=(381, 1800, 1)
[2025-09-13 14:30:00] 样本3：已从list转换为数组，形状=(426, 1800, 1)
[2025-09-13 14:30:00] 样本4：已从list转换为数组，形状=(406, 1800, 1)
[2025-09-13 14:30:00] 样本5：已从list转换为数组，形状=(516, 1800, 1)
[2025-09-13 14:30:00] 样本6：已从list转换为数组，形状=(125, 1800, 1)
[2025-09-13 14:30:00] 样本7：已从list转换为数组，形状=(324, 1800, 1)
[2025-09-13 14:30:00] 样本8：已从list转换为数组，形状=(202, 1800, 1)
[2025-09-13 14:30:00] 样本9：已从list转换为数组，形状=(137, 1800, 1)
[2025-09-13 14:30:00] 样本10：已从list转换为数组，形状=(276, 1800, 1)
[2025-09-13 14:30:00] 样本11：已从list转换为数组，形状=(417, 1800, 1)
[2025-09-13 14:30:00] 样本12：已从list转换为数组，形状=(455, 1800, 1)
[2025-09-13 14:30:00] 样本13：已从list转换为数组，形状=(446, 1800, 1)
[2025-09-13 14:30:00] 样本14：已从list转换为数组，形状=(518, 1800, 1)
[2025-09-13 14:30:00] 样本15：已从list转换为数组，形状=(835, 1800, 1)
[2025-09-13 14:30:00] 样本16：已从list转换为数组，形状=(144, 1800, 1)
[2025-09-13 14:30:00] 样本17：已从list转换为数组，形状=(1013, 1800, 1)
[2025-09-13 14:30:00] 样本18：已从list转换为数组，形状=(238, 1800, 1)
[2025-09-13 14:30:00] 样本19：已从list转换为数组，形状=(714, 1800, 1)
[2025-09-13 14:30:00] 样本20：已从list转换为数组，形状=(475, 1800, 1)
[2025-09-13 14:30:00] 样本21：已从list转换为数组，形状=(301, 1800, 1)
[2025-09-13 14:30:00] 样本22：已从list转换为数组，形状=(351, 1800, 1)
[2025-09-13 14:30:00] 样本23：已从list转换为数组，形状=(635, 1800, 1)
[2025-09-13 14:30:00] 样本24：已从list转换为数组，形状=(337, 1800, 1)
[2025-09-13 14:30:00] 样本25：已从list转换为数组，形状=(537, 1800, 1)
[2025-09-13 14:30:00] 样本26：已从list转换为数组，形状=(347, 1800, 1)
[2025-09-13 14:30:00] 样本27：已从list转换为数组，形状=(355, 1800, 1)
[2025-09-13 14:30:00] 样本28：已从list转换为数组，形状=(192, 1800, 1)
[2025-09-13 14:30:00] 样本29：已从list转换为数组，形状=(386, 1800, 1)
[2025-09-13 14:30:00] 样本30：已从list转换为数组，形状=(784, 1800, 1)
[2025-09-13 14:30:00] 样本31：已从list转换为数组，形状=(479, 1800, 1)
[2025-09-13 14:30:00] 样本32：已从list转换为数组，形状=(390, 1800, 1)
[2025-09-13 14:30:00] 样本33：已从list转换为数组，形状=(551, 1800, 1)
[2025-09-13 14:30:00] 样本34：已从list转换为数组，形状=(271, 1800, 1)
[2025-09-13 14:30:00] 样本35：已从list转换为数组，形状=(446, 1800, 1)
[2025-09-13 14:30:00] 样本36：已从list转换为数组，形状=(539, 1800, 1)
[2025-09-13 14:30:00] 样本37：已从list转换为数组，形状=(443, 1800, 1)
[2025-09-13 14:30:00] 样本38：已从list转换为数组，形状=(201, 1800, 1)
[2025-09-13 14:30:00] 样本39：已从list转换为数组，形状=(294, 1800, 1)
[2025-09-13 14:30:00] 样本40：已从list转换为数组，形状=(230, 1800, 1)
[2025-09-13 14:30:00] 样本41：已从list转换为数组，形状=(810, 1800, 1)
[2025-09-13 14:30:00] ===========================

[2025-09-13 14:30:00] 当前批次最长序列长度（窗口数）: 1013
[2025-09-13 14:30:01] X填充后形状: (42, 1013, 1800, 1)
[2025-09-13 14:30:01] 检测到的类别: [0, 1, 2, 3], 填充类别编号: 4
[2025-09-13 14:30:01] y填充后形状: (42, 1013)
[2025-09-13 14:30:01] X填充值替换为均值: -0.0002
[2025-09-13 14:30:02] 标准化后X统计: min=-19.2088, max=19.3730
[2025-09-13 14:30:02] 已检测到 3 个GPU，将用于分布式训练
[2025-09-13 14:30:03] 
模型初始化完成（多GPU支持），结构如下：
[2025-09-13 14:30:03] Model: "sequential_1"
[2025-09-13 14:30:03] _________________________________________________________________
[2025-09-13 14:30:03]  Layer (type)                Output Shape              Param #   
[2025-09-13 14:30:03] =================================================================
[2025-09-13 14:30:03]  time_distributed_cnn (TimeD  (None, 1013, 3200)       57152     
[2025-09-13 14:30:03]  istributed)                                                     
[2025-09-13 14:30:03]                                                                  
[2025-09-13 14:30:03]  bidirectional_gru (Bidirect  (None, 1013, 256)        2557440   
[2025-09-13 14:30:03]  ional)                                                          
[2025-09-13 14:30:03]                                                                  
[2025-09-13 14:30:03]  time_distributed_ffn (TimeD  (None, 1013, 5)          100869    
[2025-09-13 14:30:03]  istributed)                                                     
[2025-09-13 14:30:03]                                                                  
[2025-09-13 14:30:03] =================================================================
[2025-09-13 14:30:03] Total params: 2,715,461
[2025-09-13 14:30:03] Trainable params: 2,714,181
[2025-09-13 14:30:03] Non-trainable params: 1,280
[2025-09-13 14:30:03] _________________________________________________________________
[2025-09-13 14:30:03] 监控批次形状 - X: (5, 1013, 1800, 1), y: (5, 1013)
[2025-09-13 14:30:03] 使用验证集: True, 验证比例: 0.2
[2025-09-13 14:30:03] 
===== 类别权重 =====
[2025-09-13 14:30:03] 类别 0 (普通类别): 样本数=1769, 权重=2.7207
[2025-09-13 14:30:03] 类别 1 (普通类别): 样本数=6348, 权重=1.6003
[2025-09-13 14:30:03] 类别 2 (普通类别): 样本数=3399, 权重=2.1265
[2025-09-13 14:30:03] 类别 3 (普通类别): 样本数=5926, 权重=1.6556
[2025-09-13 14:30:03] 类别 4 (填充类别): 样本数=25104, 权重=0.0000
[2025-09-13 14:30:03] ====================

[2025-09-13 14:30:03] 样本权重范围: [0.0000, 2.7207]
[2025-09-13 14:30:03] 
【开始训练】样本数: 42, 批次大小: 5, GPU数量: 3
[2025-09-13 14:30:04] 
===== 系统资源监控初始化 =====
[2025-09-13 14:30:04] ===== 系统基本信息 =====
[2025-09-13 14:30:04] CPU核心数: 48 (物理核心: 24)
[2025-09-13 14:30:04] 总内存: 251.56 GB
[2025-09-13 14:30:04] 初始可用内存: 191.87 GB
[2025-09-13 14:30:04] GPU 0: NVIDIA GeForce RTX 3090
[2025-09-13 14:30:04]   总显存: 24.00 GB
[2025-09-13 14:30:04]   初始可用显存: 0.79 GB
[2025-09-13 14:30:04] GPU 1: NVIDIA GeForce RTX 3090
[2025-09-13 14:30:04]   总显存: 24.00 GB
[2025-09-13 14:30:04]   初始可用显存: 4.82 GB
[2025-09-13 14:30:04] GPU 2: NVIDIA GeForce RTX 3090
[2025-09-13 14:30:04]   总显存: 24.00 GB
[2025-09-13 14:30:04]   初始可用显存: 4.82 GB
[2025-09-13 14:30:04] ========================

[2025-09-13 14:31:56] 
===== Epoch 0 层 time_distributed_cnn 输出监控 =====
[2025-09-13 14:31:56] 形状: (5, 1013, 3200)
[2025-09-13 14:31:56] 最小值: 0.000000, 最大值: 47.936516
[2025-09-13 14:31:56] 含Inf: False
[2025-09-13 14:31:56] =========================================

[2025-09-13 14:31:56] 
===== Epoch 0 层 bidirectional_gru 输出监控 =====
[2025-09-13 14:31:56] 形状: (5, 1013, 256)
[2025-09-13 14:31:56] 最小值: -1.000000, 最大值: 1.000000
[2025-09-13 14:31:56] 含Inf: False
[2025-09-13 14:31:56] =========================================

[2025-09-13 14:31:56] 
===== Epoch 0 层 time_distributed_ffn 输出监控 =====
[2025-09-13 14:31:56] 形状: (5, 1013, 5)
[2025-09-13 14:31:56] 最小值: 0.011999, 最大值: 0.850662
[2025-09-13 14:31:56] 含Inf: False
[2025-09-13 14:31:56] =========================================

[2025-09-13 14:32:24] 梯度监控出错: Exception encountered when calling layer 'gru_cell_5' (type GRUCell).

{{function_node __wrapped__MatMul_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[2,128] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:MatMul]

Call arguments received by layer 'gru_cell_5' (type GRUCell):
  • inputs=tf.Tensor(shape=(2, 3200), dtype=float32)
  • states=('tf.Tensor(shape=(2, 128), dtype=float32)',)
  • training=True
[2025-09-13 14:32:24] 
===== Epoch 0 资源使用统计 =====
[2025-09-13 14:32:24] CPU使用率: 0.2%
[2025-09-13 14:32:24] 内存状态: 已用 56.73 GB / 总 251.56 GB / 可用 191.67 GB (76.2%)
[2025-09-13 14:32:24] GPU 0 状态: 使用率 0%, 显存(已用/总/可用): 23.21 / 24.00 / 0.79 GB, 温度 45°C
[2025-09-13 14:32:24] GPU 1 状态: 使用率 0%, 显存(已用/总/可用): 19.18 / 24.00 / 4.82 GB, 温度 45°C
[2025-09-13 14:32:24] GPU 2 状态: 使用率 0%, 显存(已用/总/可用): 19.18 / 24.00 / 4.82 GB, 温度 37°C
[2025-09-13 14:32:24] 当前进程内存使用: 11.38 GB
[2025-09-13 14:32:24] =================================

[2025-09-13 14:33:30] 训练过程出错: Graph execution error:

Detected at node 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1'
Detected at node 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1'
Detected at node 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1'
Detected at node 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1' defined at (most recent call last):
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 973, in _bootstrap
      self._bootstrap_inner()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/threading.py", line 1016, in _bootstrap_inner
      self.run()
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1249, in run_step
      outputs = model.train_step(data)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/engine/training.py", line 1054, in train_step
      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 542, in minimize
      grads_and_vars = self.compute_gradients(loss, var_list, tape)
    File "/root/miniconda3/envs/deep-sound/lib/python3.10/site-packages/keras/optimizers/optimizer.py", line 275, in compute_gradients
      grads = tape.gradient(loss, var_list)
Node: 'gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1'
4 root error(s) found.
  (0) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/else/_3698/replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_1/_1036]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_1/broadcast_weights_1/assert_broadcastable/AssertGuard/else/_3675/replica_1/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_0/_994]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/pivot_f/_3700/_1025]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (1) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/else/_3698/replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_1/_1036]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_1/broadcast_weights_1/assert_broadcastable/AssertGuard/else/_3675/replica_1/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_0/_994]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (2) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

	 [[replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/else/_3698/replica_2/broadcast_weights_1/assert_broadcastable/AssertGuard/Assert/data_1/_1036]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

  (3) RESOURCE_EXHAUSTED:  OOM when allocating tensor with shape[3200,384] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc
	 [[{{node gradient_tape/sequential_1/bidirectional_gru/forward_gru_1/while/zeros_1}}]]
Hint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.

0 successful operations.
0 derived errors ignored. [Op:__inference_train_function_2290928]